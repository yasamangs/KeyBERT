{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of embedding models to compare\n",
    "embedding_models = [\n",
    "    'paraphrase-MiniLM-L6-v2',\n",
    "    'paraphrase-MiniLM-L12-v2',\n",
    "    'all-MiniLM-L6-v2',\n",
    "    'all-MiniLM-L12-v2',\n",
    "    'paraphrase-albert-small-v2'\n",
    "]\n",
    "\n",
    "# Your best configurations\n",
    "best_config_mmr = {\n",
    "    'ngram_range': (1, 2),\n",
    "    'nr_candidates': 18,\n",
    "    'diversity': 0.4\n",
    "}\n",
    "\n",
    "best_config_maxsum = {\n",
    "    'ngram_range': (1, 2),\n",
    "    'nr_candidates': 18\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function: counts both exact and partial matches\n",
    "def evaluate_results(results_post, gold_keywords):\n",
    "    precisions, recalls, f1s = [], [], []\n",
    "    for pred, gold in zip(results_post, gold_keywords):\n",
    "        pred_set = set(pred)\n",
    "        gold_set = set(gold)\n",
    "        exact_matches = set([p for p in pred_set if p in gold_set])\n",
    "        partial_matches = set([\n",
    "            p for p in pred_set\n",
    "            if any((p in g or g in p) for g in gold_set) and p not in exact_matches\n",
    "        ])\n",
    "        total_matches = len(exact_matches) + len(partial_matches)\n",
    "        precision = total_matches / len(pred) if pred else 0\n",
    "        recall = total_matches / len(gold) if gold else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1 = sum(f1s) / len(f1s)\n",
    "    return avg_precision, avg_recall, avg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K500N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read docs and gold keywords   \n",
    "\n",
    "docs_dir = os.path.join(\"500N-KPCrowd-v1.1\", \"500N-KPCrowd-v1.1/docsutf8\")\n",
    "keys_dir = os.path.join(\"500N-KPCrowd-v1.1\", \"500N-KPCrowd-v1.1/keys\")\n",
    "doc_files = sorted(os.listdir(docs_dir))\n",
    "key_files = sorted(os.listdir(keys_dir))\n",
    "docs = []\n",
    "gold_keywords = [] \n",
    "for doc_file, key_file in zip(doc_files, key_files):\n",
    "    with open(os.path.join(docs_dir, doc_file), encoding='utf-8') as f:\n",
    "        docs.append(f.read())\n",
    "    with open(os.path.join(keys_dir, key_file), encoding='utf-8') as f:\n",
    "        gold_keywords.append([line.strip().lower() for line in f if line.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: paraphrase-MiniLM-L6-v2\n",
      "Precision_500N: 0.763\n",
      "Recall_500N: 0.100\n",
      "F1_500N: 0.170\n",
      "\n",
      "Evaluating model: paraphrase-MiniLM-L12-v2\n",
      "Precision_500N: 0.748\n",
      "Recall_500N: 0.097\n",
      "F1_500N: 0.166\n",
      "\n",
      "Evaluating model: all-MiniLM-L6-v2\n",
      "Precision_500N: 0.750\n",
      "Recall_500N: 0.098\n",
      "F1_500N: 0.168\n",
      "\n",
      "Evaluating model: all-MiniLM-L12-v2\n",
      "Precision_500N: 0.767\n",
      "Recall_500N: 0.100\n",
      "F1_500N: 0.171\n",
      "\n",
      "Evaluating model: paraphrase-albert-small-v2\n",
      "Precision_500N: 0.760\n",
      "Recall_500N: 0.100\n",
      "F1_500N: 0.170\n"
     ]
    }
   ],
   "source": [
    "for model_name in embedding_models:\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    sentence_model = SentenceTransformer(model_name)\n",
    "    kw_model = KeyBERT(model=sentence_model)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            doc,\n",
    "        )\n",
    "        keywords = [k[0] for k in keywords]\n",
    "        results.append(keywords)\n",
    "    \n",
    "    precision, recall, f1 = evaluate_results(results, gold_keywords)\n",
    "    print(f\"Precision_500N: {precision:.3f}\")\n",
    "    print(f\"Recall_500N: {recall:.3f}\")\n",
    "    print(f\"F1_500N: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Final Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_500N_mmr: 0.830\n",
      "Recall_500N_mmr: 0.106\n",
      "F1_500N_mmr: 0.182\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT(model='all-MiniLM-L12-v2')\n",
    "    \n",
    "# Choose configuration: mmr_config or maxsum_config\n",
    "config = best_config_mmr\n",
    "    \n",
    "results = []\n",
    "for doc in docs:\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        doc,\n",
    "        keyphrase_ngram_range=config['ngram_range'],\n",
    "        stop_words='english',\n",
    "        nr_candidates=config['nr_candidates'],\n",
    "        diversity=config['diversity'],\n",
    "        top_n=5\n",
    "    )\n",
    "    keywords = [k[0] for k in keywords]\n",
    "    results.append(keywords)\n",
    "    \n",
    "precision, recall, f1 = evaluate_results(results, gold_keywords)\n",
    "print(f\"Precision_500N_mmr: {precision:.3f}\")\n",
    "print(f\"Recall_500N_mmr: {recall:.3f}\")\n",
    "print(f\"F1_500N_mmr: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_500N_maxsum: 0.830\n",
      "Recall_500N_maxsum: 0.106\n",
      "F1_500N_maxsum: 0.182\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT(model='all-MiniLM-L12-v2')\n",
    "    \n",
    "# Choose configuration: mmr_config or maxsum_config\n",
    "config = best_config_maxsum\n",
    "    \n",
    "results = []\n",
    "for doc in docs:\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        doc,\n",
    "        keyphrase_ngram_range=config['ngram_range'],\n",
    "        stop_words='english',\n",
    "        nr_candidates=config['nr_candidates'],\n",
    "        top_n=5\n",
    "    )\n",
    "    keywords = [k[0] for k in keywords]\n",
    "    results.append(keywords)\n",
    "    \n",
    "precision, recall, f1 = evaluate_results(results, gold_keywords)\n",
    "print(f\"Precision_500N_maxsum: {precision:.3f}\")\n",
    "print(f\"Recall_500N_maxsum: {recall:.3f}\")\n",
    "print(f\"F1_500N_maxsum: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemEval2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read docs and gold keywords   \n",
    "\n",
    "docs_dir = os.path.join(\"SemEval2017\", \"docsutf8\")\n",
    "keys_dir = os.path.join(\"SemEval2017\", \"keys\")\n",
    "doc_files = sorted(os.listdir(docs_dir))\n",
    "key_files = sorted(os.listdir(keys_dir))\n",
    "docs = []\n",
    "gold_keywords = [] \n",
    "for doc_file, key_file in zip(doc_files, key_files):\n",
    "    with open(os.path.join(docs_dir, doc_file), encoding='utf-8') as f:\n",
    "        docs.append(f.read())\n",
    "    with open(os.path.join(keys_dir, key_file), encoding='utf-8') as f:\n",
    "        gold_keywords.append([line.strip().lower() for line in f if line.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: paraphrase-MiniLM-L6-v2\n",
      "Precision_SemEval2017: 0.832\n",
      "Recall_SemEval2017: 0.277\n",
      "F1_SemEval2017: 0.401\n",
      "\n",
      "Evaluating model: paraphrase-MiniLM-L12-v2\n",
      "Precision_SemEval2017: 0.849\n",
      "Recall_SemEval2017: 0.280\n",
      "F1_SemEval2017: 0.407\n",
      "\n",
      "Evaluating model: all-MiniLM-L6-v2\n",
      "Precision_SemEval2017: 0.865\n",
      "Recall_SemEval2017: 0.285\n",
      "F1_SemEval2017: 0.415\n",
      "\n",
      "Evaluating model: all-MiniLM-L12-v2\n",
      "Precision_SemEval2017: 0.873\n",
      "Recall_SemEval2017: 0.290\n",
      "F1_SemEval2017: 0.420\n",
      "\n",
      "Evaluating model: paraphrase-albert-small-v2\n",
      "Precision_SemEval2017: 0.840\n",
      "Recall_SemEval2017: 0.278\n",
      "F1_SemEval2017: 0.403\n"
     ]
    }
   ],
   "source": [
    "for model_name in embedding_models:\n",
    "    print(f\"\\nEvaluating model: {model_name}\")\n",
    "    sentence_model = SentenceTransformer(model_name)\n",
    "    kw_model = KeyBERT(model=sentence_model)\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    for doc in docs:\n",
    "        keywords = kw_model.extract_keywords(\n",
    "            doc,\n",
    "        )\n",
    "        keywords = [k[0] for k in keywords]\n",
    "        results.append(keywords)\n",
    "    \n",
    "    precision, recall, f1 = evaluate_results(results, gold_keywords)\n",
    "    print(f\"Precision_SemEval2017: {precision:.3f}\")\n",
    "    print(f\"Recall_SemEval2017: {recall:.3f}\")\n",
    "    print(f\"F1_SemEval2017: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your best configurations\n",
    "best_config_mmr = {\n",
    "    'ngram_range': (1, 1),\n",
    "    'nr_candidates': 18,\n",
    "    'diversity': 0.4\n",
    "}\n",
    "\n",
    "best_config_maxsum = {\n",
    "    'ngram_range': (1, 1),\n",
    "    'nr_candidates': 18\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Final Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_SemEval2017_mmr: 0.873\n",
      "Recall_SemEval2017_mmr: 0.290\n",
      "F1_SemEval2017_mmr: 0.420\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT(model='all-MiniLM-L12-v2')\n",
    "    \n",
    "# Choose configuration: mmr_config or maxsum_config\n",
    "config = best_config_mmr\n",
    "    \n",
    "results = []\n",
    "for doc in docs:\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        doc,\n",
    "        keyphrase_ngram_range=config['ngram_range'],\n",
    "        stop_words='english',\n",
    "        nr_candidates=config['nr_candidates'],\n",
    "        diversity=config['diversity'],\n",
    "        top_n=5\n",
    "    )\n",
    "    keywords = [k[0] for k in keywords]\n",
    "    results.append(keywords)\n",
    "    \n",
    "precision, recall, f1 = evaluate_results(results, gold_keywords)\n",
    "print(f\"Precision_SemEval2017_mmr: {precision:.3f}\")\n",
    "print(f\"Recall_SemEval2017_mmr: {recall:.3f}\")\n",
    "print(f\"F1_SemEval2017_mmr: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision_SemEval2017_maxsum: 0.873\n",
      "Recall_SemEval2017_maxsum: 0.290\n",
      "F1_SemEval2017_maxsum: 0.420\n"
     ]
    }
   ],
   "source": [
    "kw_model = KeyBERT(model='all-MiniLM-L12-v2')\n",
    "    \n",
    "# Choose configuration: mmr_config or maxsum_config\n",
    "config = best_config_maxsum\n",
    "    \n",
    "results = []\n",
    "for doc in docs:\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        doc,\n",
    "        keyphrase_ngram_range=config['ngram_range'],\n",
    "        stop_words='english',\n",
    "        nr_candidates=config['nr_candidates'],\n",
    "        top_n=5\n",
    "    )\n",
    "    keywords = [k[0] for k in keywords]\n",
    "    results.append(keywords)\n",
    "    \n",
    "precision, recall, f1 = evaluate_results(results, gold_keywords)\n",
    "print(f\"Precision_SemEval2017_maxsum: {precision:.3f}\")\n",
    "print(f\"Recall_SemEval2017_maxsum: {recall:.3f}\")\n",
    "print(f\"F1_SemEval2017_maxsum: {f1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
