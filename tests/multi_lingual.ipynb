{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (2.2.3)\n",
      "Requirement already satisfied: requests in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (4.67.1)\n",
      "Collecting lxml\n",
      "  Downloading lxml-6.0.0-cp39-cp39-macosx_10_9_universal2.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (from requests) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (from requests) (2025.4.26)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading lxml-6.0.0-cp39-cp39-macosx_10_9_universal2.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-6.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas requests tqdm lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Downloading en-fr TED2020 from OPUS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 3663it [00:04, 841.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Already extracted.\n",
      " Sample data ready:\n",
      "                                                  fr  \\\n",
      "0                  Ma famille vivait dans une hutte.   \n",
      "1  Pourquoi, en tant qu'architecte, vous int√©ress...   \n",
      "2  Et certains d'entre nous pourraient m√™me chanter.   \n",
      "\n",
      "                             gold_fr  \n",
      "0            [famille, vivait, dans]  \n",
      "1  [pourquoi,, tant, qu'architecte,]  \n",
      "2          [certains, d'entre, nous]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "#  Working OPUS download link\n",
    "url = \"https://object.pouta.csc.fi/OPUS-TED2020/v1/moses/en-fr.txt.zip\"\n",
    "filename = \"en-fr.txt.zip\"\n",
    "extracted_folder = \"en-fr-data\"\n",
    "\n",
    "# Step 1: Download the zip file\n",
    "if not os.path.exists(filename):\n",
    "    print(\" Downloading en-fr TED2020 from OPUS...\")\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in tqdm(r.iter_content(chunk_size=8192), desc=\"Downloading\"):\n",
    "                f.write(chunk)\n",
    "else:\n",
    "    print(\" Already downloaded.\")\n",
    "\n",
    "# Step 2: Extract it\n",
    "if not os.path.exists(extracted_folder):\n",
    "    os.makedirs(extracted_folder)\n",
    "    print(\" Extracting...\")\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extracted_folder)\n",
    "else:\n",
    "    print(\" Already extracted.\")\n",
    "\n",
    "# Step 3: Load the English and French files\n",
    "en_path = os.path.join(extracted_folder, \"TED2020.en-fr.en\")\n",
    "fr_path = os.path.join(extracted_folder, \"TED2020.en-fr.fr\")\n",
    "\n",
    "if not os.path.exists(en_path) or not os.path.exists(fr_path):\n",
    "    raise FileNotFoundError(\" Missing extracted files.\")\n",
    "\n",
    "# Load and clean\n",
    "with open(en_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    en_lines = [line.strip() for line in f.readlines()]\n",
    "with open(fr_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    fr_lines = [line.strip() for line in f.readlines()]\n",
    "\n",
    "df = pd.DataFrame({\"en\": en_lines, \"fr\": fr_lines}).dropna()\n",
    "df = df.sample(20, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Fake gold keywords\n",
    "def fake_gold(text):\n",
    "    return [w.lower() for w in text.split()[:4] if len(w) > 3]\n",
    "\n",
    "df[\"gold_fr\"] = df[\"fr\"].apply(fake_gold)\n",
    "\n",
    "print(\" Sample data ready:\")\n",
    "print(df[[\"fr\", \"gold_fr\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Samples of dataset:\n",
      "\n",
      "Sentence   : Ma famille vivait dans une hutte.\n",
      "Gold       : ['famille', 'vivait', 'dans']\n",
      "Sentence   : Pourquoi, en tant qu'architecte, vous int√©resseriez-vous √† l'espace ?\n",
      "Gold       : ['pourquoi,', 'tant', \"qu'architecte,\"]\n",
      "Sentence   : Et certains d'entre nous pourraient m√™me chanter.\n",
      "Gold       : ['certains', \"d'entre\", 'nous']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---------------------- üîç Print 3 Example Predictions ----------------------\n",
    "print(\"\\n Samples of dataset:\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"Sentence   : {df['fr'][i]}\")\n",
    "    print(f\"Gold       : {df['gold_fr'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function: counts both exact and partial matches\n",
    "def evaluate_results(results_post, gold_keywords):\n",
    "    precisions, recalls, f1s = [], [], []\n",
    "    for pred, gold in zip(results_post, gold_keywords):\n",
    "        pred_set = set(pred)\n",
    "        gold_set = set(gold)\n",
    "        exact_matches = set([p for p in pred_set if p in gold_set])\n",
    "        partial_matches = set([\n",
    "            p for p in pred_set\n",
    "            if any((p in g or g in p) for g in gold_set) and p not in exact_matches\n",
    "        ])\n",
    "        total_matches = len(exact_matches) + len(partial_matches)\n",
    "        precision = total_matches / len(pred) if pred else 0\n",
    "        recall = total_matches / len(gold) if gold else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    avg_precision = sum(precisions) / len(precisions)\n",
    "    avg_recall = sum(recalls) / len(recalls)\n",
    "    avg_f1 = sum(f1s) / len(f1s)\n",
    "    return avg_precision, avg_recall, avg_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Testing model: all-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:02<00:00,  7.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'all-MiniLM-L12-v2': {'precision': 0.33, 'recall': 0.675, 'f1': 0.42400793650793644, 'time_sec': 5.289292812347412}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from keybert import KeyBERT\n",
    "from tqdm import tqdm\n",
    "\n",
    "results_default = {}\n",
    "\n",
    "print(f\"\\n Testing model: {'all-MiniLM-L12-v2'}\")\n",
    "start_time = time.time()\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "kw_model = KeyBERT(model)\n",
    "\n",
    "predictions = []\n",
    "for sentence in tqdm(df[\"fr\"], desc=\"Extracting keywords\"):\n",
    "    keywords = kw_model.extract_keywords(sentence, stop_words=None, top_n=5)\n",
    "    extracted = [kw[0].lower() for kw in keywords]\n",
    "    predictions.append(extracted)\n",
    "\n",
    "precision, recall, f1 = evaluate_results(predictions, df[\"gold_fr\"].tolist())\n",
    "runtime = time.time() - start_time\n",
    "\n",
    "results_default['all-MiniLM-L12-v2'] = {\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1,\n",
    "    \"time_sec\": runtime\n",
    "}\n",
    "\n",
    "print(results_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLingual Embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Embedding Models ------------------\n",
    "multilingual_embedding_models = [\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\", \n",
    "    \"distiluse-base-multilingual-cased-v1\", \n",
    "    \"distiluse-base-multilingual-cased-v2\" \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing model: paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:03<00:00,  6.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing model: distiluse-base-multilingual-cased-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 11.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing model: distiluse-base-multilingual-cased-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting keywords: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:01<00:00, 10.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Summary of Model Performance:\n",
      "\n",
      "paraphrase-multilingual-MiniLM-L12-v2              | Precision: 0.330 | Recall: 0.637 | F1: 0.419 | Time: 16.0s\n",
      "distiluse-base-multilingual-cased-v1               | Precision: 0.380 | Recall: 0.729 | F1: 0.483 | Time: 5.6s\n",
      "distiluse-base-multilingual-cased-v2               | Precision: 0.330 | Recall: 0.638 | F1: 0.419 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Run Benchmark ------------------\n",
    "results = {}\n",
    "\n",
    "for model_name in multilingual_embedding_models:\n",
    "    print(f\"\\n Testing model: {model_name}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = SentenceTransformer(model_name)\n",
    "    kw_model = KeyBERT(model)\n",
    "\n",
    "    predictions = []\n",
    "    for sentence in tqdm(df[\"fr\"], desc=\"Extracting keywords\"):\n",
    "        keywords = kw_model.extract_keywords(sentence, stop_words=None, top_n=5)\n",
    "        extracted = [kw[0].lower() for kw in keywords]\n",
    "        predictions.append(extracted)\n",
    "\n",
    "    precision, recall, f1 = evaluate_results(predictions, df[\"gold_fr\"].tolist())\n",
    "    runtime = time.time() - start_time\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"time_sec\": runtime\n",
    "    }\n",
    "\n",
    "# ------------------ Summary ------------------\n",
    "print(\"\\n Summary of Model Performance:\\n\")\n",
    "for model, metrics in results.items():\n",
    "    print(f\"{model:50} | Precision: {metrics['precision']:.3f} | Recall: {metrics['recall']:.3f} | F1: {metrics['f1']:.3f} | Time: {metrics['time_sec']:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
