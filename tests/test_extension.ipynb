{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yasaman/anaconda3/envs/NLP/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy and define post-processing functions\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_filter(keywords):\n",
    "    filtered = []\n",
    "    for kw, score in keywords:\n",
    "        doc = nlp(kw)\n",
    "        if all(token.pos_ in {\"NOUN\", \"PROPN\", \"ADJ\"} for token in doc):\n",
    "            filtered.append((kw, score))\n",
    "    return filtered\n",
    "\n",
    "def entity_boost(keywords, text):\n",
    "    doc = nlp(text)\n",
    "    entities = set(ent.text for ent in doc.ents)\n",
    "    boosted = []\n",
    "    for kw, score in keywords:\n",
    "        if kw in entities:\n",
    "            boosted.append((kw, score + 0.2))\n",
    "        else:\n",
    "            boosted.append((kw, score))\n",
    "    return boosted\n",
    "\n",
    "\n",
    "def advanced_postprocess(keywords, doc_text, nlp):\n",
    "    keywords = entity_boost(keywords, doc_text)\n",
    "    keywords = pos_filter(keywords)\n",
    "\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "def precision(pred, gold):\n",
    "    pred_set = set(pred)\n",
    "    gold_set = set(gold)\n",
    "    return len(pred_set & gold_set) / len(pred_set) if pred_set else 0\n",
    "\n",
    "def recall(pred, gold):\n",
    "    pred_set = set(pred)\n",
    "    gold_set = set(gold)\n",
    "    return len(pred_set & gold_set) / len(gold_set) if gold_set else 0\n",
    "\n",
    "def f1(p, r):\n",
    "    return 2 * p * r / (p + r) if (p + r) else 0\n",
    "\n",
    "# Define partial matching metric\n",
    "def partial_match(pred, gold):\n",
    "    count = 0\n",
    "    for p in pred:\n",
    "        for g in gold:\n",
    "            if p in g or g in p:\n",
    "                count += 1\n",
    "                break\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize KeyBERT model\n",
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500N Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read docs and gold keywords   \n",
    "\n",
    "docs_dir = os.path.join(\"500N-KPCrowd-v1.1\", \"500N-KPCrowd-v1.1/docsutf8\")\n",
    "keys_dir = os.path.join(\"500N-KPCrowd-v1.1\", \"500N-KPCrowd-v1.1/keys\")\n",
    "doc_files = sorted(os.listdir(docs_dir))\n",
    "key_files = sorted(os.listdir(keys_dir))\n",
    "docs = []\n",
    "gold_keywords = [] \n",
    "for doc_file, key_file in zip(doc_files, key_files):\n",
    "    with open(os.path.join(docs_dir, doc_file), encoding='utf-8') as f:\n",
    "        docs.append(f.read())\n",
    "    with open(os.path.join(keys_dir, key_file), encoding='utf-8') as f:\n",
    "        gold_keywords.append([line.strip().lower() for line in f if line.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [15:43<00:00,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Post-processing: Precision 0.449, Recall 0.056, F1 0.096\n",
      "With Post-processing: Precision 0.454, Recall 0.048, F1 0.085\n",
      "No Post-processing Partial Match: 0.750\n",
      "With Post-processing Partial Match: 0.759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract keywords with and without post-processing for the full dataset\n",
    "N = 5  # Number of keywords to extract\n",
    "results_no_post = []\n",
    "results_post = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for doc in tqdm(docs):\n",
    "        kws_no_post = [kw for kw, _ in kw_model.extract_keywords(doc, top_n=N)]\n",
    "        kws_post = kw_model.extract_keywords(\n",
    "            doc, top_n=N,\n",
    "            postprocess=lambda kws: advanced_postprocess(kws, doc, nlp)\n",
    "        )\n",
    "        kws_post = [kw for kw, _ in kws_post]\n",
    "        results_no_post.append(kws_no_post)\n",
    "        results_post.append(kws_post)\n",
    "\n",
    "    # Evaluate and print results\n",
    "    p_no_post, r_no_post, f1_no_post = [], [], []\n",
    "    p_post, r_post, f1_post = [], [], []\n",
    "\n",
    "    pm_no_post, pm_post = [], []\n",
    "\n",
    "\n",
    "    for pred, gold, pred_post in zip(results_no_post, gold_keywords, results_post):\n",
    "        # Exact match\n",
    "        p = precision(pred, gold)\n",
    "        r = recall(pred, gold)\n",
    "        f = f1(p, r)\n",
    "        p_no_post.append(p)\n",
    "        r_no_post.append(r)\n",
    "        f1_no_post.append(f)\n",
    "\n",
    "        p2 = precision(pred_post, gold)\n",
    "        r2 = recall(pred_post, gold)\n",
    "        f2 = f1(p2, r2)\n",
    "        p_post.append(p2)\n",
    "        r_post.append(r2)\n",
    "        f1_post.append(f2)\n",
    "        # Partial match\n",
    "        pm_no_post.append(partial_match(pred, gold) / len(pred) if pred else 0)\n",
    "        pm_post.append(partial_match(pred_post, gold) / len(pred_post) if pred_post else 0)\n",
    "\n",
    "    print(\"No Post-processing: Precision {:.3f}, Recall {:.3f}, F1 {:.3f}\".format(\n",
    "        sum(p_no_post)/len(p_no_post), sum(r_no_post)/len(r_no_post), sum(f1_no_post)/len(f1_no_post)))\n",
    "    print(\"With Post-processing: Precision {:.3f}, Recall {:.3f}, F1 {:.3f}\".format(\n",
    "        sum(p_post)/len(p_post), sum(r_post)/len(r_post), sum(f1_post)/len(f1_post)))\n",
    "    print(\"No Post-processing Partial Match: {:.3f}\".format(sum(pm_no_post)/len(pm_no_post)))\n",
    "    print(\"With Post-processing Partial Match: {:.3f}\".format(sum(pm_post)/len(pm_post)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SemEval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# read docs and gold keywords   \n",
    "\n",
    "docs_dir = os.path.join(\"SemEval2017\", \"docsutf8\")\n",
    "keys_dir = os.path.join(\"SemEval2017\", \"keys\")\n",
    "doc_files = sorted(os.listdir(docs_dir))\n",
    "key_files = sorted(os.listdir(keys_dir))\n",
    "docs = []\n",
    "gold_keywords = [] \n",
    "for doc_file, key_file in zip(doc_files, key_files):\n",
    "    with open(os.path.join(docs_dir, doc_file), encoding='utf-8') as f:\n",
    "        docs.append(f.read())\n",
    "    with open(os.path.join(keys_dir, key_file), encoding='utf-8') as f:\n",
    "        gold_keywords.append([line.strip().lower() for line in f if line.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/493 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 493/493 [03:00<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Post-processing: Precision 0.200, Recall 0.059, F1 0.089\n",
      "With Post-processing: Precision 0.202, Recall 0.049, F1 0.076\n",
      "No Post-processing Partial Match: 0.865\n",
      "With Post-processing Partial Match: 0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract keywords with and without post-processing for the full dataset\n",
    "N = 5  # Number of keywords to extract\n",
    "results_no_post = []\n",
    "results_post = []\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for doc in tqdm(docs):\n",
    "        kws_no_post = [kw for kw, _ in kw_model.extract_keywords(doc, top_n=N)]\n",
    "        kws_post = kw_model.extract_keywords(\n",
    "            doc, top_n=N,\n",
    "            postprocess=lambda kws: advanced_postprocess(kws, doc, nlp)\n",
    "        )\n",
    "        kws_post = [kw for kw, _ in kws_post]\n",
    "        results_no_post.append(kws_no_post)\n",
    "        results_post.append(kws_post)\n",
    "\n",
    "    # Evaluate and print results\n",
    "    p_no_post, r_no_post, f1_no_post = [], [], []\n",
    "    p_post, r_post, f1_post = [], [], []\n",
    "\n",
    "    pm_no_post, pm_post = [], []\n",
    "\n",
    "\n",
    "    for pred, gold, pred_post in zip(results_no_post, gold_keywords, results_post):\n",
    "        # Exact match\n",
    "        p = precision(pred, gold)\n",
    "        r = recall(pred, gold)\n",
    "        f = f1(p, r)\n",
    "        p_no_post.append(p)\n",
    "        r_no_post.append(r)\n",
    "        f1_no_post.append(f)\n",
    "\n",
    "        p2 = precision(pred_post, gold)\n",
    "        r2 = recall(pred_post, gold)\n",
    "        f2 = f1(p2, r2)\n",
    "        p_post.append(p2)\n",
    "        r_post.append(r2)\n",
    "        f1_post.append(f2)\n",
    "        # Partial match\n",
    "        pm_no_post.append(partial_match(pred, gold) / len(pred) if pred else 0)\n",
    "        pm_post.append(partial_match(pred_post, gold) / len(pred_post) if pred_post else 0)\n",
    "\n",
    "    print(\"No Post-processing: Precision {:.3f}, Recall {:.3f}, F1 {:.3f}\".format(\n",
    "        sum(p_no_post)/len(p_no_post), sum(r_no_post)/len(r_no_post), sum(f1_no_post)/len(f1_no_post)))\n",
    "    print(\"With Post-processing: Precision {:.3f}, Recall {:.3f}, F1 {:.3f}\".format(\n",
    "        sum(p_post)/len(p_post), sum(r_post)/len(r_post), sum(f1_post)/len(f1_post)))\n",
    "    print(\"No Post-processing Partial Match: {:.3f}\".format(sum(pm_no_post)/len(pm_no_post)))\n",
    "    print(\"With Post-processing Partial Match: {:.3f}\".format(sum(pm_post)/len(pm_post)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
